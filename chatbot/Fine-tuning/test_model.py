#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test du Mod√®le FSBM LLaMA Fine-tun√©
====================================

Ce script teste le mod√®le fine-tun√© pour v√©rifier qu'il fonctionne correctement.
"""

import os
import torch
import json
import time
from pathlib import Path

# Configuration
MODEL_PATH = "Model"  # Chemin vers le dossier du mod√®le
HF_TOKEN = "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"

def print_header():
    """Afficher l'en-t√™te du test"""
    print("=" * 60)
    print("üß™ TEST DU MOD√àLE FSBM LLaMA FINE-TUN√â")
    print("=" * 60)
    print()

def check_model_files():
    """V√©rifier que tous les fichiers du mod√®le sont pr√©sents"""
    print("üîç V√âRIFICATION DES FICHIERS...")
    
    essential_files = [
        "adapter_model.bin",
        "adapter_config.json",
        "tokenizer.json",
        "tokenizer_config.json",
        "special_tokens_map.json",
        "chat_template.jinja",
        "README.md",
        "model_info.json",
        "training_args.bin",
        "tokenizer.model"
    ]
    
    missing_files = []
    for file in essential_files:
        file_path = os.path.join(MODEL_PATH, file)
        if os.path.exists(file_path):
            size = os.path.getsize(file_path) / (1024 * 1024)  # MB
            print(f"‚úÖ {file} ({size:.1f} MB)")
        else:
            print(f"‚ùå {file} - MANQUANT")
            missing_files.append(file)
    
    # V√©rifier les checkpoints
    checkpoint_dirs = [d for d in os.listdir(MODEL_PATH) if d.startswith("checkpoint-")]
    if checkpoint_dirs:
        print(f"‚úÖ Checkpoints trouv√©s: {len(checkpoint_dirs)}")
        for checkpoint in checkpoint_dirs:
            print(f"  üìÅ {checkpoint}/")
    
    if missing_files:
        print(f"\n‚ùå FICHIERS MANQUANTS: {len(missing_files)}")
        return False
    
    print("\n‚úÖ TOUS LES FICHIERS SONT PR√âSENTS!")
    return True

def load_model_info():
    """Charger les informations du mod√®le"""
    print("\nüìä INFORMATIONS DU MOD√àLE...")
    
    model_info_path = os.path.join(MODEL_PATH, "model_info.json")
    if os.path.exists(model_info_path):
        try:
            with open(model_info_path, 'r', encoding='utf-8') as f:
                model_info = json.load(f)
            
            print(f"üéØ Nom: {model_info.get('model_name', 'N/A')}")
            print(f"üîß Mod√®le de base: {model_info.get('base_model', 'N/A')}")
            print(f"üìà √âpoques: {model_info.get('training_config', {}).get('epochs', 'N/A')}")
            print(f"üìö Dataset: {model_info.get('dataset', 'N/A')}")
            print(f"‚è±Ô∏è Temps d'entra√Ænement: {model_info.get('training_time_seconds', 0):.1f}s")
            print(f"üíæ Quantization: {model_info.get('quantization', 'N/A')}")
            print(f"üîß Framework: {model_info.get('framework', 'N/A')}")
            
            return model_info
        except Exception as e:
            print(f"‚ùå Erreur lecture model_info.json: {e}")
            return None
    else:
        print("‚ùå Fichier model_info.json non trouv√©")
        return None

def setup_environment():
    """Configurer l'environnement"""
    print("\nüîß CONFIGURATION DE L'ENVIRONNEMENT...")
    
    # D√©sactiver les warnings
    os.environ["WANDB_DISABLED"] = "true"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    # V√©rifier GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üñ•Ô∏è Device: {device}")
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("üßπ M√©moire GPU nettoy√©e")
        print(f"üíæ M√©moire GPU disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    return device

def load_model_and_tokenizer():
    """Charger le mod√®le et le tokenizer"""
    print("\nüöÄ CHARGEMENT DU MOD√àLE ET TOKENIZER...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
        from peft import PeftModel
        from huggingface_hub import login
        
        # Authentification Hugging Face
        print("üîê Authentification Hugging Face...")
        login(token=HF_TOKEN)
        print("‚úÖ Authentification r√©ussie!")
        
        # Configuration 8-bit
        print("üîß Configuration 8-bit...")
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=False,
        )
        
        # Charger le tokenizer
        print("üìù Chargement du tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        print("‚úÖ Tokenizer charg√©!")
        
        # Charger le mod√®le
        print("üß† Chargement du mod√®le...")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        print("‚úÖ Mod√®le charg√©!")
        
        return model, tokenizer
        
    except ImportError as e:
        print(f"‚ùå Erreur d'import: {e}")
        print("üí° Installez les d√©pendances: pip install transformers peft torch bitsandbytes")
        return None, None
    except Exception as e:
        print(f"‚ùå Erreur de chargement: {e}")
        return None, None

def test_generation(model, tokenizer, prompt, max_tokens=100):
    """Tester la g√©n√©ration de texte"""
    try:
        # Formater le prompt
        formatted_prompt = f"<s>[INST] {prompt} [/INST]"
        inputs = tokenizer.encode(formatted_prompt, return_tensors="pt")
        
        # D√©placer sur le bon device
        device = next(model.parameters()).device
        inputs = inputs.to(device)
        
        # G√©n√©rer
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # D√©coder la r√©ponse
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        if "[/INST]" in response:
            response = response.split("[/INST]")[1].strip()
        
        return response
        
    except Exception as e:
        return f"‚ùå Erreur de g√©n√©ration: {e}"

def run_tests(model, tokenizer):
    """Ex√©cuter les tests"""
    print("\nüß™ TESTS DE G√âN√âRATION...")
    print("=" * 50)
    
    # Questions de test
    test_questions = [
        "Qu'est-ce que le Big Data?",
        "Expliquez Python",
        "Qu'est-ce que l'IA?",
        "D√©finissez la Business Intelligence",
        "Qu'est-ce que le Machine Learning?",
        "Expliquez les bases de donn√©es",
        "Qu'est-ce que l'analyse de donn√©es?",
        "D√©finissez le Data Mining"
    ]
    
    results = []
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n--- Test {i}/{len(test_questions)} ---")
        print(f"üìù Question: {question}")
        
        start_time = time.time()
        response = test_generation(model, tokenizer, question)
        generation_time = time.time() - start_time
        
        print(f"ü§ñ R√©ponse: {response}")
        print(f"‚è±Ô∏è Temps: {generation_time:.2f}s")
        
        results.append({
            "question": question,
            "response": response,
            "time": generation_time
        })
    
    return results

def analyze_results(results):
    """Analyser les r√©sultats des tests"""
    print("\nüìä ANALYSE DES R√âSULTATS...")
    print("=" * 50)
    
    total_time = sum(r["time"] for r in results)
    avg_time = total_time / len(results)
    
    print(f"üìà Nombre de tests: {len(results)}")
    print(f"‚è±Ô∏è Temps total: {total_time:.2f}s")
    print(f"‚è±Ô∏è Temps moyen: {avg_time:.2f}s")
    
    # V√©rifier la qualit√© des r√©ponses
    good_responses = 0
    for result in results:
        response = result["response"]
        if not response.startswith("‚ùå") and len(response) > 10:
            good_responses += 1
    
    success_rate = (good_responses / len(results)) * 100
    print(f"‚úÖ Taux de succ√®s: {success_rate:.1f}%")
    
    if success_rate >= 80:
        print("üéâ EXCELLENT! Le mod√®le fonctionne parfaitement!")
    elif success_rate >= 60:
        print("‚úÖ BON! Le mod√®le fonctionne bien!")
    else:
        print("‚ö†Ô∏è ATTENTION! Le mod√®le a des probl√®mes.")

def main():
    """Fonction principale"""
    print_header()
    
    # V√©rifier les fichiers
    if not check_model_files():
        print("‚ùå Fichiers manquants. Arr√™t du test.")
        return
    
    # Charger les informations du mod√®le
    model_info = load_model_info()
    
    # Configurer l'environnement
    device = setup_environment()
    
    # Charger le mod√®le et tokenizer
    model, tokenizer = load_model_and_tokenizer()
    
    if model is None or tokenizer is None:
        print("‚ùå Impossible de charger le mod√®le. Arr√™t du test.")
        return
    
    # Ex√©cuter les tests
    results = run_tests(model, tokenizer)
    
    # Analyser les r√©sultats
    analyze_results(results)
    
    print("\n" + "=" * 60)
    print("üéâ TEST TERMIN√â!")
    print("=" * 60)
    print("‚úÖ Le mod√®le a √©t√© test√© avec succ√®s")
    print("üìä R√©sultats disponibles ci-dessus")
    print("üöÄ Votre mod√®le FSBM LLaMA est pr√™t √† l'emploi!")

if __name__ == "__main__":
    main()
