#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chat Interactif CPU Corrig√© - Mod√®le FSBM LLaMA Fine-tun√©
==========================================================

Version corrig√©e du chat interactif pour r√©soudre les probl√®mes d'attention mask.
"""

import os
import torch
import json
from pathlib import Path

# Configuration
MODEL_PATH = "Model"
HF_TOKEN = "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"

def print_header():
    """Afficher l'en-t√™te du chat"""
    print("=" * 60)
    print("üéì FSBM SCHOLAR ASSISTANT - CHAT INTERACTIF (CPU CORRIG√â)")
    print("=" * 60)
    print("ü§ñ Assistant IA fine-tun√© pour les questions FSBM")
    print("üíª Mode CPU - Version corrig√©e")
    print("üí° Tapez 'help' pour voir les questions d'exemple")
    print("üö™ Tapez 'quit' pour quitter")
    print("=" * 60)
    print()

def load_model():
    """Charger le mod√®le fine-tun√© sur CPU"""
    print("üîß Chargement du mod√®le sur CPU...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from huggingface_hub import login
        
        # Configuration
        os.environ["WANDB_DISABLED"] = "true"
        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        
        # Authentification
        login(token=HF_TOKEN)
        
        # Charger le tokenizer
        print("üìù Chargement du tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True
        )
        
        # Configuration du tokenizer
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # S'assurer que le tokenizer a un token de padding diff√©rent
        if tokenizer.pad_token == tokenizer.eos_token:
            # Cr√©er un token de padding unique
            tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        
        print("‚úÖ Tokenizer charg√©!")
        
        # Charger le mod√®le sur CPU
        print("üß† Chargement du mod√®le (CPU)...")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            device_map="cpu",  # Forcer CPU
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            torch_dtype=torch.float32  # Utiliser float32 pour CPU
        )
        
        # Redimensionner l'embedding si n√©cessaire
        if tokenizer.pad_token != tokenizer.eos_token:
            model.resize_token_embeddings(len(tokenizer))
        
        print("‚úÖ Mod√®le charg√© avec succ√®s sur CPU!")
        return model, tokenizer
        
    except Exception as e:
        print(f"‚ùå Erreur de chargement: {e}")
        return None, None

def generate_response(model, tokenizer, prompt, max_tokens=150):
    """G√©n√©rer une r√©ponse avec attention mask corrig√©"""
    try:
        # Formater le prompt
        formatted_prompt = f"<s>[INST] {prompt} [/INST]"
        
        # Tokenization avec attention mask
        inputs = tokenizer(
            formatted_prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512,
            add_special_tokens=True
        )
        
        # Forcer sur CPU
        inputs = {k: v.to("cpu") for k, v in inputs.items()}
        
        # G√©n√©rer avec attention mask
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                attention_mask=inputs['attention_mask']
            )
        
        # D√©coder la r√©ponse
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extraire seulement la partie r√©ponse
        if "[/INST]" in response:
            response = response.split("[/INST]")[1].strip()
        
        return response
        
    except Exception as e:
        return f"‚ùå Erreur de g√©n√©ration: {e}"

def show_help():
    """Afficher l'aide et les questions d'exemple"""
    print("\nüìö QUESTIONS D'EXEMPLE:")
    print("-" * 40)
    
    sample_questions = [
        "Qu'est-ce que le Big Data?",
        "Expliquez les 3 V du Big Data",
        "Qu'est-ce que la Business Intelligence?",
        "D√©finissez le Machine Learning",
        "Qu'est-ce que l'analyse de donn√©es?",
        "Expliquez les bases de donn√©es",
        "Qu'est-ce que le Data Mining?",
        "D√©finissez l'Intelligence Artificielle",
        "Qu'est-ce qu'Apache Hadoop?",
        "Expliquez Python pour la data science",
        "Qu'est-ce que la validation crois√©e?",
        "D√©finissez l'ing√©nierie des caract√©ristiques"
    ]
    
    for i, question in enumerate(sample_questions, 1):
        print(f"{i:2d}. {question}")
    
    print("\nüí° COMMANDES SP√âCIALES:")
    print("- help : Afficher cette aide")
    print("- quit : Quitter le chat")
    print("- clear : Effacer l'√©cran")
    print("- info : Informations sur le mod√®le")

def show_model_info():
    """Afficher les informations du mod√®le"""
    print("\nüìä INFORMATIONS DU MOD√àLE:")
    print("-" * 40)
    
    model_info_path = os.path.join(MODEL_PATH, "model_info.json")
    if os.path.exists(model_info_path):
        try:
            with open(model_info_path, 'r', encoding='utf-8') as f:
                model_info = json.load(f)
            
            print(f"üéØ Nom: {model_info.get('model_name', 'N/A')}")
            print(f"üîß Mod√®le de base: {model_info.get('base_model', 'N/A')}")
            print(f"üìà √âpoques: {model_info.get('training_config', {}).get('epochs', 'N/A')}")
            print(f"üìö Dataset: {model_info.get('dataset', 'N/A')}")
            print(f"‚è±Ô∏è Temps d'entra√Ænement: {model_info.get('training_time_seconds', 0):.1f}s")
            print(f"üíæ Quantization: {model_info.get('quantization', 'N/A')}")
            print(f"üîß Framework: {model_info.get('framework', 'N/A')}")
            print(f"üíª Mode: CPU (version corrig√©e)")
            
        except Exception as e:
            print(f"‚ùå Erreur lecture model_info.json: {e}")
    else:
        print("‚ùå Fichier model_info.json non trouv√©")

def clear_screen():
    """Effacer l'√©cran"""
    os.system('cls' if os.name == 'nt' else 'clear')
    print_header()

def interactive_chat():
    """Interface de chat interactive corrig√©e"""
    print_header()
    
    # Charger le mod√®le
    model, tokenizer = load_model()
    
    if model is None or tokenizer is None:
        print("‚ùå Impossible de charger le mod√®le. Arr√™t du chat.")
        return
    
    print("\nüéâ Pr√™t pour le chat! Posez vos questions FSBM...")
    print("‚ö†Ô∏è Mode CPU - Les r√©ponses peuvent √™tre plus lentes")
    print("üîß Version corrig√©e - Attention mask g√©r√©")
    print()
    
    # Statistiques
    question_count = 0
    total_time = 0
    
    while True:
        try:
            # Obtenir l'entr√©e utilisateur
            user_input = input("\nü§î Vous: ").strip()
            
            # Commandes sp√©ciales
            if user_input.lower() == 'quit':
                print("\nüëã Au revoir! Merci d'avoir test√© le mod√®le FSBM!")
                if question_count > 0:
                    avg_time = total_time / question_count
                    print(f"üìä Statistiques: {question_count} questions, temps moyen: {avg_time:.2f}s")
                break
                
            elif user_input.lower() == 'help':
                show_help()
                continue
                
            elif user_input.lower() == 'info':
                show_model_info()
                continue
                
            elif user_input.lower() == 'clear':
                clear_screen()
                continue
                
            elif user_input.lower() == '':
                continue
            
            # G√©n√©rer la r√©ponse
            print("ü§ñ Assistant: ", end="", flush=True)
            
            import time
            start_time = time.time()
            response = generate_response(model, tokenizer, user_input)
            generation_time = time.time() - start_time
            
            print(response)
            print(f"‚è±Ô∏è Temps: {generation_time:.2f}s")
            
            # Mettre √† jour les statistiques
            question_count += 1
            total_time += generation_time
            
        except KeyboardInterrupt:
            print("\n\nüëã Au revoir! Merci d'avoir test√© le mod√®le FSBM!")
            if question_count > 0:
                avg_time = total_time / question_count
                print(f"üìä Statistiques: {question_count} questions, temps moyen: {avg_time:.2f}s")
            break
            
        except Exception as e:
            print(f"\n‚ùå Erreur: {e}")
            print("üí° Essayez une autre question ou tapez 'help' pour voir les exemples")

def main():
    """Fonction principale"""
    try:
        interactive_chat()
    except Exception as e:
        print(f"‚ùå Erreur fatale: {e}")
        print("üí° V√©rifiez que le mod√®le est correctement install√©")

if __name__ == "__main__":
    main()
