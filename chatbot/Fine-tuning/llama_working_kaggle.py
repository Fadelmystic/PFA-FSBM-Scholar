# LLaMA Working - Solution D√©finitive pour Kaggle avec PEFT
# ========================================================

import os
import torch
import time
import json
import subprocess
import sys
import shutil
import zipfile

# Configuration
os.environ["WANDB_DISABLED"] = "true"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# V√©rification et installation de bitsandbytes pour Kaggle
print("üîß V√©rification de bitsandbytes...")
try:
    import bitsandbytes
    print("‚úÖ bitsandbytes d√©j√† install√©!")
except ImportError:
    print("üì¶ Installation de bitsandbytes...")
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-U", "bitsandbytes"])
        print("‚úÖ bitsandbytes install√© avec succ√®s!")
        print("üîÑ Red√©marrage du runtime n√©cessaire...")
        print("‚ö†Ô∏è Veuillez red√©marrer le runtime Kaggle et relancer ce script")
        raise RuntimeError("Red√©marrage du runtime Kaggle n√©cessaire apr√®s installation de bitsandbytes")
    except Exception as e:
        print(f"‚ùå Erreur lors de l'installation: {e}")
        raise

# Installation de PEFT si n√©cessaire
print("üîß V√©rification de PEFT...")
try:
    import peft
    print("‚úÖ PEFT d√©j√† install√©!")
except ImportError:
    print("üì¶ Installation de PEFT...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "peft"])
    print("‚úÖ PEFT install√©!")

# Maintenant on peut importer les modules
try:
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer,
        DataCollatorForLanguageModeling, BitsAndBytesConfig
    )
    from datasets import Dataset, load_dataset
    from huggingface_hub import login
    from peft import LoraConfig, get_peft_model, TaskType
    print("‚úÖ Tous les modules import√©s avec succ√®s!")
except ImportError as e:
    print(f"‚ùå Erreur d'import: {e}")
    raise

# Configuration LLaMA avec optimisations m√©moire
MODEL_NAME = "meta-llama/Llama-2-7b-chat-hf"  # Version chat plus optimis√©e
OUTPUT_DIR = "fsbm-llama-working"
MAX_LENGTH = 256  # Plus long pour de meilleures r√©ponses
BATCH_SIZE = 1    # Batch minimal
LEARNING_RATE = 1e-4  # Learning rate plus bas pour plus d'√©poques
NUM_EPOCHS = 8   # Plus d'√©poques pour un meilleur fine-tuning

# Token Hugging Face
HF_TOKEN = "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"

# Authentification
print("üîê Authentification Hugging Face...")
login(token=HF_TOKEN)
print("‚úÖ Authentification r√©ussie!")

# GPU check et configuration m√©moire
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"üñ•Ô∏è Device: {device}")

if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print("üßπ M√©moire GPU nettoy√©e")
    
    # Configuration m√©moire avanc√©e - FORCER cuda:0
    torch.cuda.set_device(0)
    print("üéØ GPU 0 configur√©")
    
    # Forcer CUDA_VISIBLE_DEVICES
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    print("üîí CUDA_VISIBLE_DEVICES fix√© √† 0")

# Charger les donn√©es depuis Hugging Face
print("üìÇ Chargement des donn√©es depuis Hugging Face...")
dataset_hf = load_dataset("faduul/fsbm-qa-dataset")
data = dataset_hf["train"]
print(f"‚úÖ {len(data)} exemples charg√©s")

# Format LLaMA optimis√©
def format_llama_data(data):
    formatted = []
    for item in data:
        prompt = item["prompt"].strip()
        response = item["response"].strip()
        # Format LLaMA chat optimis√©
        text = f"<s>[INST] {prompt} [/INST] {response} </s>"
        formatted.append({"text": text})
    return formatted

# Pr√©paration des donn√©es
print("üîÑ Formatage des donn√©es...")
formatted_data = format_llama_data(data)
dataset = Dataset.from_list(formatted_data)
print(f"‚úÖ Donn√©es format√©es: {len(formatted_data)} exemples")

# Tokenizer avec token
print(f"üîß Initialisation tokenizer: {MODEL_NAME}")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    token=HF_TOKEN,
    trust_remote_code=True
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Tokenization
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=MAX_LENGTH,
        return_tensors="pt"
    )

print("üî§ Tokenisation...")
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=dataset.column_names
)
print("‚úÖ Tokenisation termin√©e")

# Split train/validation
dataset_size = len(tokenized_dataset)
train_size = int(0.9 * dataset_size)
train_dataset = tokenized_dataset.select(range(train_size))
val_dataset = tokenized_dataset.select(range(train_size, dataset_size))
print(f"üìä Train: {len(train_dataset)}, Val: {len(val_dataset)}")

# Configuration 8-bit pour √©viter l'erreur CUBLAS
print("üîß Configuration 8-bit...")
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,  # Utiliser 8-bit au lieu de 4-bit
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False,
)

# Mod√®le avec optimisations m√©moire
print("üöÄ Chargement du mod√®le LLaMA avec optimisations...")
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    token=HF_TOKEN,
    quantization_config=bnb_config,
    device_map={"": 0},  # FORCER tout sur cuda:0
    trust_remote_code=True,
    low_cpu_mem_usage=True
)

# Configuration LoRA pour PEFT
print("üîß Configuration LoRA...")
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16,  # Rank
    lora_alpha=32,  # Alpha parameter
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    bias="none",
)

# Appliquer LoRA au mod√®le
print("üîß Application de LoRA...")
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# Training arguments ultra-optimis√©s pour LLaMA avec PEFT
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    overwrite_output_dir=True,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    learning_rate=LEARNING_RATE,
    warmup_steps=20,  # Plus de warmup pour plus d'√©poques
    logging_steps=10,  # Plus de logging pour suivre le progr√®s
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,  # Garder plus de checkpoints
    fp16=True,
    remove_unused_columns=False,
    report_to=[],
    gradient_accumulation_steps=8,  # Moins d'accumulation pour plus de mises √† jour
    optim="paged_adamw_8bit",  # Optimiseur 8-bit
    lr_scheduler_type="cosine",
    weight_decay=0.01,
    max_grad_norm=1.0,
    dataloader_num_workers=0,
    gradient_checkpointing=False,  # D√©sactiv√© pour √©viter les conflits de device
    ddp_find_unused_parameters=False,
    dataloader_pin_memory=False,
    save_safetensors=False,
    torch_compile=False,
)

# Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator,
)

# Entra√Ænement
print("üéØ D√©marrage entra√Ænement LLaMA avec PEFT...")
start_time = time.time()
trainer.train()
training_time = time.time() - start_time

# Sauvegarder
trainer.save_model()
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"\nüìä R√âSULTATS:")
print(f"‚è±Ô∏è Temps: {training_time:.1f}s")
print(f"‚úÖ Mod√®le sauvegard√©: {OUTPUT_DIR}")

# Test simple
print(f"\nüß™ Test du mod√®le LLaMA...")
test_model = AutoModelForCausalLM.from_pretrained(
    OUTPUT_DIR,
    quantization_config=bnb_config,
    device_map={"": 0},  # FORCER tout sur cuda:0
    trust_remote_code=True
)
test_model.eval()

def test_generation(prompt):
    formatted_prompt = f"<s>[INST] {prompt} [/INST]"
    inputs = tokenizer.encode(formatted_prompt, return_tensors="pt")
    
    # FORCER sur cuda:0
    if torch.cuda.is_available():
        inputs = inputs.to("cuda:0")
    
    with torch.no_grad():
        outputs = test_model.generate(
            inputs,
            max_new_tokens=100,  # Plus de tokens pour des r√©ponses plus longues
            do_sample=True,      # Activation du sampling pour plus de vari√©t√©
            temperature=0.7,     # Temp√©rature pour la cr√©ativit√©
            top_p=0.9,           # Top-p sampling
            repetition_penalty=1.1,  # √âviter la r√©p√©tition
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "[/INST]" in response:
        response = response.split("[/INST]")[1].strip()
    return response

# Tests
test_questions = [
    "Qu'est-ce que le Big Data?",
    "Expliquez Python",
    "Qu'est-ce que l'IA?"
]

print("\nüß™ TESTS LLaMA:")
for i, question in enumerate(test_questions, 1):
    print(f"\n--- Test {i} ---")
    print(f"üìù Question: {question}")
    response = test_generation(question)
    print(f"ü§ñ R√©ponse: {response}")

# ========================================================
# T√âL√âCHARGEMENT DU MOD√àLE FINE-TUN√â POUR KAGGLE
# ========================================================

print(f"\nüíæ PR√âPARATION DU T√âL√âCHARGEMENT...")

def create_model_info():
    """Cr√©er un fichier d'information sur le mod√®le"""
    model_info = {
        "model_name": "FSBM-Llama-2-7b-chat-finetuned",
        "base_model": MODEL_NAME,
        "training_config": {
            "epochs": NUM_EPOCHS,
            "learning_rate": LEARNING_RATE,
            "batch_size": BATCH_SIZE,
            "max_length": MAX_LENGTH,
            "lora_config": {
                "r": 16,
                "lora_alpha": 32,
                "lora_dropout": 0.1
            }
        },
        "dataset": "faduul/fsbm-qa-dataset",
        "training_time_seconds": training_time,
        "quantization": "8-bit",
        "framework": "PEFT LoRA"
    }
    
    with open(f"{OUTPUT_DIR}/model_info.json", "w", encoding="utf-8") as f:
        json.dump(model_info, f, indent=2, ensure_ascii=False)
    
    print("‚úÖ Fichier model_info.json cr√©√©")

def create_readme():
    """Cr√©er un README pour le mod√®le"""
    readme_content = f"""# FSBM LLaMA-2-7b-chat Fine-tuned Model

## Description
Mod√®le LLaMA-2-7b-chat fine-tun√© sur le dataset FSBM (Fili√®re Sciences et Technologies du Management et de la Business Intelligence) pour r√©pondre aux questions en fran√ßais.

## Configuration d'entra√Ænement
- **Mod√®le de base**: {MODEL_NAME}
- **√âpoques**: {NUM_EPOCHS}
- **Learning rate**: {LEARNING_RATE}
- **Batch size**: {BATCH_SIZE}
- **Longueur max**: {MAX_LENGTH} tokens
- **Quantization**: 8-bit
- **Framework**: PEFT LoRA

## Utilisation

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Charger le tokenizer
tokenizer = AutoTokenizer.from_pretrained("{OUTPUT_DIR}")

# Charger le mod√®le
model = AutoModelForCausalLM.from_pretrained("{OUTPUT_DIR}")

# G√©n√©rer une r√©ponse
prompt = "<s>[INST] Votre question ici [/INST]"
inputs = tokenizer.encode(prompt, return_tensors="pt")
outputs = model.generate(inputs, max_new_tokens=100)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

## Performance
- Temps d'entra√Ænement: {training_time:.1f} secondes
- Dataset: faduul/fsbm-qa-dataset
- Optimisations: 8-bit quantization, PEFT LoRA

## Auteur
G√©n√©r√© automatiquement avec LLaMA Working pour Kaggle
"""
    
    with open(f"{OUTPUT_DIR}/README.md", "w", encoding="utf-8") as f:
        f.write(readme_content)
    
    print("‚úÖ README.md cr√©√©")

def compress_model():
    """Compresser le mod√®le pour le t√©l√©chargement"""
    print("üóúÔ∏è Compression du mod√®le...")
    
    # Cr√©er les fichiers d'information
    create_model_info()
    create_readme()
    
    # Nom du fichier zip
    zip_filename = "fsbm-llama-finetuned-model.zip"
    
    # Supprimer l'ancien zip s'il existe
    if os.path.exists(zip_filename):
        os.remove(zip_filename)
        print("üóëÔ∏è Ancien fichier zip supprim√©")
    
    # Cr√©er le nouveau zip
    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(OUTPUT_DIR):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, OUTPUT_DIR)
                zipf.write(file_path, arcname)
                print(f"üì¶ Ajout√©: {arcname}")
    
    # V√©rifier la taille
    zip_size = os.path.getsize(zip_filename) / (1024 * 1024)  # MB
    print(f"‚úÖ Mod√®le compress√©: {zip_filename} ({zip_size:.1f} MB)")
    
    return zip_filename

def download_for_kaggle():
    """Pr√©parer le t√©l√©chargement pour Kaggle"""
    print("\nüöÄ PR√âPARATION DU T√âL√âCHARGEMENT KAGGLE...")
    
    # Compresser le mod√®le
    zip_filename = compress_model()
    
    # Cr√©er un script de t√©l√©chargement
    download_script = f"""#!/bin/bash
# Script de t√©l√©chargement pour Kaggle
echo "üì• T√©l√©chargement du mod√®le FSBM LLaMA fine-tun√©..."

# V√©rifier que le fichier existe
if [ -f "{zip_filename}" ]; then
    echo "‚úÖ Fichier trouv√©: {zip_filename}"
    echo "üìä Taille: $(du -h {zip_filename} | cut -f1)"
    echo "üéØ Le fichier est pr√™t pour le t√©l√©chargement!"
    echo "üí° Utilisez le bouton 'Download' dans Kaggle pour t√©l√©charger {zip_filename}"
else
    echo "‚ùå Fichier non trouv√©: {zip_filename}"
fi
"""
    
    with open("download_model.sh", "w") as f:
        f.write(download_script)
    
    # Rendre le script ex√©cutable
    os.chmod("download_model.sh", 0o755)
    
    print("‚úÖ Script de t√©l√©chargement cr√©√©: download_model.sh")
    
    # Afficher les instructions
    print(f"\nüìã INSTRUCTIONS DE T√âL√âCHARGEMENT:")
    print(f"1. Le mod√®le est sauvegard√© dans: {OUTPUT_DIR}/")
    print(f"2. Le fichier compress√© est: {zip_filename}")
    print(f"3. Pour t√©l√©charger, utilisez le bouton 'Download' dans Kaggle")
    print(f"4. Ou ex√©cutez: !bash download_model.sh")
    
    # Lister les fichiers disponibles
    print(f"\nüìÅ FICHIERS DISPONIBLES:")
    if os.path.exists(OUTPUT_DIR):
        print(f"üìÇ Dossier mod√®le: {OUTPUT_DIR}/")
        for root, dirs, files in os.walk(OUTPUT_DIR):
            level = root.replace(OUTPUT_DIR, '').count(os.sep)
            indent = ' ' * 2 * level
            print(f"{indent}üìÅ {os.path.basename(root)}/")
            subindent = ' ' * 2 * (level + 1)
            for file in files:
                print(f"{subindent}üìÑ {file}")
    
    if os.path.exists(zip_filename):
        zip_size = os.path.getsize(zip_filename) / (1024 * 1024)
        print(f"üì¶ Fichier zip: {zip_filename} ({zip_size:.1f} MB)")
    
    print(f"\nüéâ T√âL√âCHARGEMENT PR√äT!")
    print(f"üíæ Le mod√®le fine-tun√© est maintenant disponible pour t√©l√©chargement")

# Ex√©cuter le t√©l√©chargement
download_for_kaggle()

print(f"\nüéâ LLaMA WORKING TERMIN√â!")
print(f"üíæ Optimisations appliqu√©es:")
print(f"- 8-bit quantization (√©vite erreur CUBLAS)")
print(f"- PEFT LoRA adapters")
print(f"- Gradient accumulation 8x")
print(f"- Paged AdamW 8-bit")
print(f"- Gradient checkpointing d√©sactiv√©")
print(f"- Low CPU memory usage")
print(f"- {NUM_EPOCHS} √©poques de fine-tuning")
print(f"- S√©quences de {MAX_LENGTH} tokens")
print(f"- G√©n√©ration avec sampling cr√©atif")
print(f"‚úÖ LLaMA fonctionne maintenant!")
print(f"üì• Mod√®le pr√™t pour t√©l√©chargement!")
