#!/usr/bin/env python3
"""
Pipeline Automatis√© d'Extraction de Concepts pour FSBM Scholar Assistant
Surveille automatiquement les nouveaux documents et relance l'extraction
"""

import os
import json
import hashlib
import time
import logging
from pathlib import Path
from typing import Dict, List, Set, Optional
from datetime import datetime
import schedule
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from dynamic_concept_extractor import DynamicConceptExtractor
from enhanced_kg_chatbot import EnhancedKGQueryEngine
from neo4j_manager import Neo4jManager

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('auto_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DocumentChangeHandler(FileSystemEventHandler):
    """Gestionnaire des changements de documents"""
    
    def __init__(self, pipeline):
        self.pipeline = pipeline
        self.last_processed = {}
        
    def on_created(self, event):
        if not event.is_directory and event.src_path.endswith('.pdf'):
            logger.info(f"üìÑ Nouveau document d√©tect√©: {event.src_path}")
            self.pipeline.schedule_extraction()
            
    def on_modified(self, event):
        if not event.is_directory and event.src_path.endswith('.pdf'):
            logger.info(f"üìù Document modifi√©: {event.src_path}")
            self.pipeline.schedule_extraction()
            
    def on_deleted(self, event):
        if not event.is_directory and event.src_path.endswith('.pdf'):
            logger.info(f"üóëÔ∏è Document supprim√©: {event.src_path}")
            self.pipeline.schedule_extraction()

class AutoConceptPipeline:
    """Pipeline automatis√© d'extraction de concepts"""
    
    def __init__(self, docs_path: str = "../docs", config_file: str = "pipeline_config.json"):
        self.docs_path = Path(docs_path)
        self.config_file = config_file
        
        # Utiliser l'extracteur robuste si disponible
        try:
            from robust_pdf_extractor import RobustPDFExtractor
            self.extractor = DynamicConceptExtractor()
            self.robust_extractor = RobustPDFExtractor()
            logger.info("‚úÖ Extracteur PDF robuste disponible")
        except ImportError:
            self.extractor = DynamicConceptExtractor()
            self.robust_extractor = None
            logger.info("‚ö†Ô∏è Extracteur PDF robuste non disponible, utilisation de l'extracteur standard")
        
        self.neo4j_manager = None
        self.observer = None
        self.change_handler = None
        
        # Configuration par d√©faut
        self.config = self.load_config()
        
        # √âtat du pipeline
        self.last_extraction = None
        self.extraction_scheduled = False
        self.processing = False
        
        # Initialiser la base de donn√©es si configur√©e
        if self.config.get("use_neo4j", False):
            try:
                neo4j_config = self.config.get("neo4j_config", {})
                self.neo4j_manager = Neo4jManager(
                    uri=neo4j_config.get("uri"),
                    user=neo4j_config.get("user"),
                    password=neo4j_config.get("password")
                )
                logger.info("‚úÖ Connexion Neo4j √©tablie")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Impossible de se connecter √† Neo4j: {e}")
                logger.info("üí° Le pipeline continuera sans Neo4j")
                self.neo4j_manager = None
        
        logger.info("üöÄ Pipeline d'extraction automatique initialis√©")
        
    def load_config(self) -> Dict:
        """Charge la configuration du pipeline"""
        default_config = {
            "auto_extract": True,
            "extraction_interval": 3600,  # 1 heure
            "watch_directory": True,
            "use_neo4j": True,
            "backup_results": True,
            "max_concepts_per_module": 100,
            "min_concept_frequency": 2,
            "extraction_timeout": 300,  # 5 minutes
            "notify_changes": True
        }
        
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    default_config.update(user_config)
                    logger.info("‚úÖ Configuration charg√©e")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur de chargement de la config: {e}")
        
        # Sauvegarder la configuration par d√©faut
        self.save_config(default_config)
        return default_config
    
    def save_config(self, config: Dict):
        """Sauvegarde la configuration"""
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"‚ùå Erreur de sauvegarde de la config: {e}")
    
    def get_documents_hash(self) -> str:
        """Calcule le hash de tous les documents pour d√©tecter les changements"""
        all_hashes = []
        
        for pdf_file in self.docs_path.rglob("*.pdf"):
            try:
                stat = pdf_file.stat()
                file_hash = f"{pdf_file.name}:{stat.st_mtime}:{stat.st_size}"
                all_hashes.append(file_hash)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur avec {pdf_file}: {e}")
        
        # Trier pour avoir un hash coh√©rent
        all_hashes.sort()
        combined = "|".join(all_hashes)
        return hashlib.md5(combined.encode()).hexdigest()
    
    def has_documents_changed(self) -> bool:
        """V√©rifie si les documents ont chang√© depuis la derni√®re extraction"""
        current_hash = self.get_documents_hash()
        
        if not self.last_extraction:
            return True
            
        last_hash = self.last_extraction.get("documents_hash")
        return current_hash != last_hash
    
    def extract_concepts(self) -> Dict:
        """Lance l'extraction de concepts"""
        if self.processing:
            logger.info("‚è≥ Extraction d√©j√† en cours, ignor√©e")
            return None
            
        self.processing = True
        start_time = time.time()
        
        try:
            logger.info("üîç Lancement de l'extraction de concepts...")
            
            # Extraire tous les concepts
            results = self.extractor.extract_all_concepts_from_docs(self.docs_path)
            
            # Ajouter les m√©tadonn√©es
            results["extraction_metadata"] = {
                "timestamp": datetime.now().isoformat(),
                "documents_hash": self.get_documents_hash(),
                "processing_time": time.time() - start_time,
                "total_documents": len(results.get("documents", [])),
                "total_concepts": sum(len(concepts) for concepts in results.get("concepts", {}).values())
            }
            
            # Sauvegarder les r√©sultats
            self.save_extraction_results(results)
            
            # Mettre √† jour la base de donn√©es si configur√©e
            if self.neo4j_manager and self.config.get("use_neo4j"):
                self.update_neo4j_database(results)
            
            # Sauvegarder l'√©tat
            self.last_extraction = results["extraction_metadata"]
            
            logger.info(f"‚úÖ Extraction termin√©e en {time.time() - start_time:.2f}s")
            logger.info(f"üìä {results['extraction_metadata']['total_concepts']} concepts extraits")
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'extraction: {e}")
            return None
        finally:
            self.processing = False
    
    def save_extraction_results(self, results: Dict):
        """Sauvegarde les r√©sultats d'extraction"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Sauvegarde principale
        main_file = f"extraction_results_{timestamp}.json"
        try:
            with open(main_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"‚úÖ R√©sultats sauvegard√©s: {main_file}")
        except Exception as e:
            logger.error(f"‚ùå Erreur de sauvegarde: {e}")
        
        # Sauvegarde de sauvegarde si configur√©e
        if self.config.get("backup_results", True):
            backup_file = f"backup/extraction_results_{timestamp}.json"
            try:
                os.makedirs("backup", exist_ok=True)
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
                logger.info(f"‚úÖ Sauvegarde cr√©√©e: {backup_file}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erreur de sauvegarde: {e}")
        
        # Mettre √† jour le fichier principal
        try:
            with open("latest_extraction_results.json", 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur de mise √† jour du fichier principal: {e}")
    
    def update_neo4j_database(self, results: Dict):
        """Met √† jour la base de donn√©es Neo4j avec les nouveaux concepts"""
        if not self.neo4j_manager:
            return
            
        try:
            logger.info("üîÑ Mise √† jour de la base Neo4j...")
            
            # Cr√©er les n≈ìuds de concepts
            for module, concepts in results.get("concepts", {}).items():
                for concept in concepts:
                    self.neo4j_manager.create_concept_node(concept, module)
            
            # Cr√©er les relations
            for relation in results.get("relations", []):
                self.neo4j_manager.create_relation(
                    relation["source"], 
                    relation["target"], 
                    relation["type"]
                )
            
            logger.info("‚úÖ Base Neo4j mise √† jour")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur de mise √† jour Neo4j: {e}")
    
    def schedule_extraction(self):
        """Planifie une extraction si pas d√©j√† planifi√©e"""
        if not self.extraction_scheduled:
            self.extraction_scheduled = True
            logger.info("‚è∞ Extraction planifi√©e dans 30 secondes...")
            
            # Planifier l'extraction dans 30 secondes
            schedule.every(30).seconds.do(self.run_scheduled_extraction).tag('extraction')
    
    def run_scheduled_extraction(self):
        """Ex√©cute l'extraction planifi√©e"""
        if self.extraction_scheduled:
            self.extraction_scheduled = False
            schedule.clear('extraction')
            
            if self.has_documents_changed():
                logger.info("üîÑ Changements d√©tect√©s, lancement de l'extraction...")
                self.extract_concepts()
            else:
                logger.info("‚úÖ Aucun changement d√©tect√©")
    
    def start_watching(self):
        """D√©marre la surveillance du r√©pertoire"""
        if not self.config.get("watch_directory", True):
            return
            
        try:
            self.observer = Observer()
            self.change_handler = DocumentChangeHandler(self)
            
            self.observer.schedule(
                self.change_handler, 
                str(self.docs_path), 
                recursive=True
            )
            
            self.observer.start()
            logger.info(f"üëÄ Surveillance d√©marr√©e sur {self.docs_path}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur de d√©marrage de la surveillance: {e}")
    
    def start_scheduled_extraction(self):
        """D√©marre l'extraction planifi√©e"""
        if not self.config.get("auto_extract", True):
            return
            
        interval = self.config.get("extraction_interval", 3600)
        schedule.every(interval).seconds.do(self.run_scheduled_extraction)
        logger.info(f"‚è∞ Extraction planifi√©e toutes les {interval} secondes")
    
    def run(self):
        """Lance le pipeline complet"""
        logger.info("üöÄ D√©marrage du pipeline automatique...")
        
        # Premi√®re extraction si n√©cessaire
        if self.has_documents_changed():
            logger.info("üîÑ Premi√®re extraction...")
            self.extract_concepts()
        
        # D√©marrer la surveillance
        self.start_watching()
        
        # D√©marrer l'extraction planifi√©e
        self.start_scheduled_extraction()
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(1)
                
        except KeyboardInterrupt:
            logger.info("üõë Arr√™t du pipeline...")
            self.stop()
    
    def stop(self):
        """Arr√™te le pipeline"""
        if self.observer:
            self.observer.stop()
            self.observer.join()
        
        logger.info("‚úÖ Pipeline arr√™t√©")
    
    def get_status(self) -> Dict:
        """Retourne le statut du pipeline"""
        return {
            "status": "running" if not self.processing else "processing",
            "last_extraction": self.last_extraction,
            "extraction_scheduled": self.extraction_scheduled,
            "documents_hash": self.get_documents_hash(),
            "config": self.config
        }

def main():
    """Fonction principale"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Pipeline automatique d'extraction de concepts")
    parser.add_argument("--docs", default="../docs", help="Chemin vers les documents")
    parser.add_argument("--config", default="pipeline_config.json", help="Fichier de configuration")
    parser.add_argument("--extract-now", action="store_true", help="Lancer une extraction imm√©diatement")
    parser.add_argument("--status", action="store_true", help="Afficher le statut")
    
    args = parser.parse_args()
    
    pipeline = AutoConceptPipeline(args.docs, args.config)
    
    if args.status:
        status = pipeline.get_status()
        print(json.dumps(status, indent=2, ensure_ascii=False))
        return
    
    if args.extract_now:
        pipeline.extract_concepts()
        return
    
    # Lancer le pipeline complet
    try:
        pipeline.run()
    except KeyboardInterrupt:
        pipeline.stop()

if __name__ == "__main__":
    main()
